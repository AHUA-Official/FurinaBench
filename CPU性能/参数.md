# Linux内核参数--VM篇

内核的VM参数涉及到pagecahe、OOM、Swap、hugepages、numa等。

## VM参数介绍

### kernel.randomize_va_space

地址空间布局随机化，设置为0将禁用ASLR。

需要注意的是部分应用程序因为依赖固定地址的共享内存对象，需要关闭ASLR。

### vm.overcommit_memory & vm.overcommit_ratio

overcommit是可以接受的，因为进程可能申请了大量的内存，但是没有完全使用，只有进程真正要写内存时才会为其分配物理内存，所以应该允许分配比物理内存更多的内存。

但是不允许无限制的分配，Linux支持使用`vm.overcommit_memory`来设置overcommit策略。

**overcommit默认值为0**，Heuristic overcommit handling。基本上能应对大部分情况，除了一些特别夸张的内存申请，一般的内存申请都会被允许。例如申请的内存超过剩余内存+swap-共享内存-保留内存时就会被拒绝，返回ENOMEM。

**overcommit值为1**，比较适合科学计算，例如稀疏矩阵，虚拟内存中很多内容都是0。

这种情况下任何内存分配请求都会成功。

**overcommit值为2**，此设置会严格统计内存使用情况，并且只有在物理内存可用时才会允许内存申请。该检查在内存分配时完成，请求内存的程序可以正常处理该错误。

overcommit 2会保留一部分内存给内核使用，分配的数量由`vm.overcommit_ratio`配置，这就表示可用于程序的虚拟内存实际为RAN*(overcommit_ratio/100)+Swap。需要注意的是`vm.overcommit_ratio`不能设置的太高，还需要保留内存用于IO缓冲区和系统调用等。

发生Page Fault时，如果没有足够的可用物理内存，系统将触发OOM Killer。OOM Killer将选择当前使用内存最多的进程，并将杀死该进程。通常无法预测何时会OOM Killer，也无法预测会终止哪些进程。

参考[overcommit该怎么设置？](https://ieevee.com/tech/2017/09/10/overcommit.html)

### vm.dirty_background_ratio & vm.dirty_background_bytes

当系统脏页的比例或者所占内存数量超过 dirty_background_ratio或dirty_background_bytes设定的阈值时，启动相关内核线程(pdflush/flush/kdmflush)开始将脏页写入磁盘。

如果该值过大，同时又有进程大量写磁盘(未使用DIRECT_IO)时，会使pagecache占用对应比例的系统内存。

注意：dirty_background_bytes参数和dirty_background_ratio参数是相对的，只能指定其中一个。当其中一个参数文件被写入时，会立即开始计算脏页限制，并且会将另一个参数的值清零。

### vm.dirty_ratio & vm.dirty_bytes

当系统pagecache的脏页达到系统内存  dirty_ratio或dirty_bytes阈值时，系统就会阻塞新的写请求，直到脏页被回写到磁盘，此值过低时，遇到写入突增时，会造成短时间内pagecache脏页快速上升，造成写请求耗时增加。但是此值也不能设置的太高，当该值太高时，会造成内核flush脏页时，超过内核限制的120s导致进程挂起或中断。 注意：dirty_bytes 和 dirty_ratio  是相对的，只能指定其中一个。当其中一个参数文件被写入时，会立即开始计算脏页限制，并且会将另一个参数的值清零。

### vm.vfs_cache_pressure

表示内核回收用于directory和inode cache内存的倾向；缺省值100表示内核将根据pagecache和swapcache，把directory和inode  cache保持在一个合理的百分比；降低该值低于100，将导致内核倾向于保留directory和inode  cache；增加该值超过100，将导致内核倾向于回收directory和inode cache。

### vm.swappiness

`vm.swappiness`参数值可设置范围在0到100之间，控制换出运行时内存的相对权重，即内存使用在(100-swappiness)%时就会有使用交换分区的情况。

`vm.swappiness`=0表示仅在内存不足(剩余空闲内存低于`vm.min_free_kbytes`时)的情况下使用交换空间，最大限度的使用物理内存；`vm.swappiness`=1表示进行最小程度的内存交换，但不禁用；`vm.swappiness`=100表示积极使用交换空间；**内存足够时推荐设置为0**。

### vm.min_free_kbytes

改参数控制系统保留空闲内存的最低值，该值设置过大可能会导致linux触发内存回收的阈值高，再叠加available无可用内存但free还有空闲内存，这种情况下没有可用内存也不回收内存，造成内存申请失败--Cannot allocate memory。

如果设置过小就会频繁导致内核进行direct  reclaim(直接回收)，即直接在应用程序的进程上下文中进行回收，再用回收上来的空闲页满足内存申请，因为是需要先回收然后再分配，所以会阻塞应用程序，带来一定的响应延迟；设置过小会相对更频繁的触发direct reclaim。

min_free_kbytes=sqrt(lowmem_kbytes*16)=4*sqrt(lowmem_kbytes)，lowmem_kbytes即可认为是系统内存大小。如果没有在/etc/sysctl.conf设置，那么默认计算出来的值有最小最大限制，最小为128K，最大为64M。设置过大会引发内存浪费，并且频繁触发oom，前提是`vm.panic_on_oom` =0。

> `vm.panic_on_oom`=0系统会提示oom，并启动oom-killer杀掉占用最高内存的进程。

### vm.panic_on_oom

控制内核在OOM时是否panic。设置为0时，内核会启动OOM-KILLER杀死内存占用过多的进程。通常杀死内存占用最多的进程，系统就会恢复。**设置为1时，内核会panic**。如果一个进程通过内存策略或进程绑定限制了可以使用的节点，并且这些节点的内存已经 耗尽，OOM-KILLER可能会杀死一个进程来释放内存。在这种情况下，内核不会panic，因为其他节点的内存可能还有空闲，这意味着整个系统的内存状况还没有处于崩溃状态。

### vm.page-cluster

控制一次从SWAP中连续读取多少内存页，当使用SWAP时，较低的该值，能减少一次读取SWAP的数据，而从减少延时。

**注意**：这个配置不是指多少页，而是2的多少次幂，当值为0时，是1页，值为1时，是2页，默认值是3，是8页。

### vm.dirty_expire_centisecs

声明Linux内核写缓冲区里面的数据多“旧”了之后，pdflush/flush/kdmflush进程就开始考虑写到磁盘中去。对于特别重的写操作来说，这个值适当缩小也是好的，但也不能缩小太多，因为缩小太多也会导致IO提高太快。单位为0.01s，默认值为3000。

### kernel.shmmax & kernel.shmall & kernel.shmmni

`kernel.shmmax`定义了共享内存段的最大大小(byte)，`kernel.shmall`定义了系统上可以使用的共享内存的大小(byte)，`kernel.shmmni`定义了共享内存段的最大数量，默认为4096。

通常不需要修改，部分场景可能需要修改，例如Oracle数据库。

### vm.drop_caches

释放cache，该参数每修改一次，触发一次释放操作。可以设置的值为1、2、3，分别表示：

1 = 清除pagecache；

2 = 清除回收slab分配器中的缓存对象，包括目录项缓存以及inode缓存；

3 = 清除pagecache和slab分配器中的缓存对象；

建议执行操作前先执行`sync`。

> 一般情况下，系统稳定运行时，free值也会保持在一个稳定的值，虽然可能看上去很小。当发生内存不足、应用获取不到可用内存、OOM错误时，应去分析应用方面的原因，如用户量太大导致的内存不足、内存溢出等情况，否则清空buffer，强制腾出free的大小，可能只是把问题屏蔽了。其次内核是可以快速清空cache和buffer的，但是并没有这样做(kernel优先把内存都用上，cache住)，我们就不应该强制去变更。

### vm.oom_dump_tasks

启用OOM信息输出。当OOM-Killer启动时，会将各进程的pid、uid、tgid、vm、size、rss、pgtables_bytes、swapents、oom_score_adj等信息打印到dmesg里，用于定位问题。

### vm.oom_kill_allocating_task

控制是否kill掉触发OOM的进程。设置为0时，OOM-Killer会扫描进程列表，选择一个进程来干掉，通常是消耗内存最多的进程，这样可以释放大量的内存。设置为非0时只会简单地将触发OOM的进程干掉，避免遍历进程列表，减少决策开销。建议设置为0。

> 为了避免特殊的进程被OOM-Killer干掉，可以修改进程的oom_score_adj(echo -1000 > /proc/{pid}/oom_score_adj)，禁止OOM-Killer杀掉该进程。

### vm.watermark_scale_factor

水位线等级系数，这个系数控制了kswapd进程的激进程度，控制了kswapd进程从唤醒到休眠，需要给系统(NUMA节点)是释放出多少内存。  该值的单位是万分几。默认值是10，意思是0.1%的系统内存(NUMA节点内存)。该值的最大值是1000，意思是10%的系统内存(NUMA节点内存)。

### vm.zone_reclaim_mode

该参数在NUMA架构下才有效，用于控制NUMA node上内存区域OOM时，如何来回收内存。如果设置为0(默认值)，则禁用node内存回收，从系统其他NUMA node上分配内存。该值还可以为以下值：

1 = 启用node内内存回收

2 = 刷脏页回收内存

4 = 通过swap回收内存

对于一般的服务应该设置为0，因为它们通常能从cache中获益，将数据cache住远比数据NUMA节点内存本地化重要。当用户的服务确定能很好的利用NUMA节点内存本地化带来的好处时，再启动该参数。

### vm.min_slab_ratio

该参数只在NUMA架构中才有效，不建议随意修改。  如果一个内存区域中可以回收的slab页面所占的百分比(应该是相对于当前内存域的所有页面)超过min_slab_ratio，在回收区的slabs会被回收。这样可以确保即使在很少执行全局回收的NUMA架构中，slab的增长也是可控的。

### vm.min_unmapped_ratio

NUMA架构下有效，只有在当前内存域中处于zone_reclaim_mode允许回收状态的内存页所占的百分比超过min_unmapped_ratio时，内存域才会执行回收操作。

### vm.nr_hugepages

hugepage池大小，单位是page，默认是0，等于关闭hugepage功能。

设置大于0的值，开启hugepage功能，开启后，内核会预分配hugepage内存到池子中去，然后其他程序就可以使用hugepage功能了。也可以通过`cat /proc/meminfo |grep HugePages_Total`来查看详情。

### vm.nr_overcommit_hugepages

设置hugepage可以超额使用的page，hugepage池的实际大小为nr_hugepages+nr_overcommit_hugepages，当vm.nr_overcommit_hugepages>0时，可能会使用到Swap。

### vm.admin_reserve_kbytes

给admin预留的内存，用于紧急登录、执行命令等，避免系统内存紧张时，admin用户无法登录，无法查看问题。

### vm.user_reserve_kbytes

当overcommit_memory设置为2，never overcommit模式时，预留[3%的进程内存大小]和[user_reserve_kbytes]两者之间最小的数量的内存。  该值的作用是，避免内核将剩余空闲内存一次性分配给一个新启动的进程，造成系统空闲内存耗尽。将该值减少到0，将会允许内核将剩余空闲内存一次性分配给一个申请内存的进程，但是可能带来负面影响。

### vm.mmap_min_addr

该参数定义了用户进程能够映射的最低内存地址。由于最开始的几个内存页面用于处理内核空引用错误，这些页面不允许写入。 该参数的默认值是0，表示安全模块不需要强制保护最开始的页面。 如果设置为64K，可以保证大多数的程序运行正常，避免比较隐蔽的问题。

### vm.max_map_count

每个进程内存拥有的VMA(虚拟内存区域)的数量。

虚拟内存区域是一个连续的虚拟地址空间区域。在进程的生命周期中，每当程序尝试在内存中映射文件，链接到共享内存段，或者分配堆空间的时候，这些区域将被创建。进程加载的动态库、分配的内存、mmap的内存都会增加VMA的数量。通常一个进程会有小于1K个VMA，如果进程有特殊逻辑，可能会超过该限制。 调优这个值将限制进程可拥有VMA的数量。

限制一个进程拥有VMA的总数可能导致应用程序出错，因为当进程达到了VMA上线但又只能释放少量的内存给其他的内核进程使用时，操作系统会抛出内存不足的错误。如果你的操作系统在NORMAL区域仅占用少量的内存，那么调低这个值可以帮助释放内存给内核用。可以参考[vm.max_map_count -- ElasticSearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html).

### vm.extfrag_threshold

当向系统申请一大段连续内存时，如果找不到符合条件的连续页，是回收内存还是压缩内存来释放空间。它是一个0 到1000的整数，默认为500。

在debugfs下面的extfrag/extfrag_index文件显示的是系统中每个zone中的每一个order的碎片编号是什么。值趋于0意味着内存分配会因为内存不足而失败，值是1000是指分配失败是因为碎片，值是-1则指主要水位满足，就能分配成功。如果碎片index 小于等于extfrag_threshold，那么内核不会压缩一个zone里面的内存。